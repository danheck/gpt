% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/test_X2.R
\name{test.gpt}
\alias{test.gpt}
\title{Goodness of Fit for GPT Models}
\usage{
test.gpt(model, breaks = "Sturges", statistic = "dn", lambda = 1,
  bootstrap = 100)
}
\arguments{
\item{model}{fitted GPT model (\link{fit.gpt}).}

\item{breaks}{a vector giving the breakpoints or other arguments passed to \code{\link[graphics]{hist}}.
Can be a list of arguments if multivariate continuous data are modelled.}

\item{statistic}{a vector with labels of the statistic to be computed (using partial matching).
\itemize{
\item \code{"dn"} the Dzhaparidze-Nikulin statistic
\item \code{"pf"} the Pearson-Fisher test (refitting the model to the binned data)
\item \code{"pd"} the power-divergence statistic with parameter \code{lambda}. 
    Since the asymptotic distribution is not chi^2, this statistic requires a 
    parametric or nonparametric bootstrap (not implemented).
}}

\item{lambda}{Only relevant for \code{statistic = "pf"} and \code{"pd"}. 
Lambda is the parameter of the power-divergence statistic by Read & Cressie (1988).
\code{lambda=1} is equivalent to Pearson's X^2, and \code{lambda=0} is
equivalent to the likelihood-ratio statistic G^2.}

\item{bootstrap}{number of parametric bootstrap samples.}
}
\description{
Continuous variables are categorized into discrete bins to compute Pearson's
X^2 between the predicted and observed bin frequencies.
}
\details{
For the number of cells, D'Agostino & Stephens (1986; p.70) recommend 2*n^2/5 where
n is then number of observations for a univariate continuous random variable.
}
\references{
D'Agostino, R. B., & Stephens, M. A. (1986). Goodness-of-fit techniques. New York, NY: Marcel Dekker, Inc.
}
