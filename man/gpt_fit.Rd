% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_gpt_user.R
\name{gpt_fit}
\alias{gpt_fit}
\title{Fit GPT Model}
\usage{
gpt_fit(x, y, data, file, latent, group = NULL, restrictions = NULL,
  baseline = FALSE, starting.values = NULL, eta.lower = NULL,
  eta.upper = NULL, n.fit = c(3, 1), maxit = c(300, 1000),
  EM.tol = 0.01, cpu = NULL, print = FALSE)
}
\arguments{
\item{x}{vector of observed discrete observations (e.g., choices) or name of variable in \code{data}}

\item{y}{vector  or matrix of observed continuous variable(s) (e.g., response times, confidence,...) or name of variable in \code{data}}

\item{data}{data frame containing the variables as named in \code{x} and \code{y}}

\item{file}{a character vector specifying the path to the model file}

\item{latent}{type of latent continuous distribution (one of \code{"normal"}, \code{"exgauss"}, \code{"exwald"}, \code{"gamma"}, \code{"weibull"}, \code{"lognormal"}, \code{"wald"}, or \code{beta}). Can be a  vector if multiple continuous variables \code{y} have different distributions (e.g., \code{latent = c("normal", "gamma")})}

\item{group}{vector with the number of observations as in \code{x} and \code{y}, indicating to which group each observation belongs}

\item{restrictions}{list with parameter restrictions (e.g., \code{list("g=0.5", "mean1=mean2=100")})}

\item{baseline}{whether to fit baseline models that assume a saturated MPT structure and (a) separate continuous distributions per category (\code{y.per.cat} = no mixture) or (b) a single distribution across all categories (\code{y.null}). In each case, separate distributions are assumed for each continuous variable.}

\item{starting.values}{starting values for theta and eta (used only in first EM run). Note that sensible starting values are guessed by default.}

\item{eta.lower}{lower bound vector or scalar for eta parameters (assigned by name; or in alphabetical order). It is sufficient to constrain a subset of eta parameters selectively using a named vector.}

\item{eta.upper}{upper bound vector or scalar for eta parameters}

\item{n.fit}{number of EM and optim fitting runs (if any \code{n.fit>1}: with random starting values)}

\item{maxit}{maximum number of EM and optim iterations, respectively}

\item{EM.tol}{tolerance for EM algorithm}

\item{cpu}{number of cores used to fit individual data (default: number of cores minus one). Alternatively: a cluster spawned by \code{makeCluster()}}

\item{print}{whether to print current status}
}
\description{
Fit GPT Model
}
\details{
The following paramaters are used to specify the latent distributions (will be assigned in this order to the parameters listed in the model file as last argument in each row/branch):
\itemize{
\item{\code{"normal"}:}{mean (mu) and SD (sigma)}
\item{\code{"exgauss"} (sum of normal and independent exponential):}{mean and SD of normal (mu, sigma); mean of exponential (tau)}
\item{\code{"gamma"}:}{ shape (k), scale (theta), shift}
\item{\code{"weibull"}:}{shape (k), scale (lambda), shift}
\item{\code{"lognormal"}:}{mean and SD (of normal distribution before taking log), shift}
\item{\code{"wald"} (= inverse normal):}{mean (mu), shape (lambda), shift}
\item{\code{"exwald"} (= sum of inverse normal and independent exponential):}{mean, shape, shift}
\item{\code{"beta"}:}{shape1 (alpha) and shape2 (beta), see \link{dbeta}}
\item{\code{"vonmises"}:}{mu (mean), kappa (concentration) of von Mises distribution for circular data. }
}
}
\examples{
\dontrun{
n <- c(targets=75, lures=75)     # number of items
theta <- c(do=.6,dn=.4, g=.5)          # MPT parameters
eta <- c(mu=400, sig=50, lambda_do=300, 
         lambda_go=500, lambda_gn=500, 
         lambda_dn=300)          # exGaussian parameters

file <- paste0(path.package("gpt"), "/models/2htm_exgauss.txt")
gen <- gpt_gen(n=n, theta=theta, eta=eta, latent="exgauss", file=file)
fit <- gpt_fit(x=gen$x, y=gen$y, latent="exgauss", file=file, 
               restrictions=list("do=dn", "lambda_do=lambda_dn", 
                                 "lambda_go=lambda_gn"))
fit
}
}
