% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/test_fit.R
\name{test_fit}
\alias{test_fit}
\title{Goodness of Fit for GPT Models}
\usage{
test_fit(gpt_fit, breaks, bins = 6, statistic = "dn", lambda = 1)
}
\arguments{
\item{gpt_fit}{a fitted GPT model (see \code{\link{gpt_fit}})}

\item{breaks}{a list giving the breakpoints per category or a vector, in 
which case the same bounds are used for each category. By default,
model-implied quantiles are used for each category.}

\item{bins}{number of bins used to compute model-implied boundaries/quantiles.}

\item{statistic}{a vector with labels of the statistic to be computed.
\itemize{
  \item \code{"dn"} the Dzhaparidze-Nikulin statistic
  \item \code{"pf"} the Pearson-Fisher test (refitting the model to the binned data)
  \item \code{"pd"} the power-divergence statistic with parameter \code{lambda}. 
      Since the asymptotic distribution is not chi^2, this statistic requires a 
      parametric or nonparametric bootstrap (not implemented).
}}

\item{lambda}{Only relevant for \code{statistic = "pf"} and \code{"pd"}. 
Lambda is the parameter of the power-divergence statistic by Read & Cressie (1988).
\code{lambda=1} is equivalent to Pearson's X^2, and \code{lambda=0} is
equivalent to the likelihood-ratio statistic G^2.}
}
\description{
The continuous variable of a GPT model is categorized into discrete bins to compute Pearsons
X^2 between the predicted and observed bin frequencies.
}
\references{
Dzhaparidze, K., & Nikulin, M. (1974). 
   On a modification of the standard statistics of Pearson. 
   Theory of Probability & Its Applications, 19(4), 851-853. \url{https://doi.org/10.1137/1119098}

Read, T. R. C., & Cressie, N. A. C. (1988). 
  Goodness-of-fit statistics for discrete multivariate data. New York, NY: Springer.

D'Agostino, R. B., & Stephens, M. A. (1986). Goodness-of-fit techniques. New York, NY: Marcel Dekker, Inc.
}
